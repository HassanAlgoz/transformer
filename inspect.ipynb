{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f00bf318830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import utils\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch_directml\n",
    "import model.net as net\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "seed = 230\n",
    "\n",
    "# select the GPU device if available\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "# elif torch_directml.is_available():\n",
    "#     device = torch_directml.device(torch_directml.default_device())\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join('experiments', 'base_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_kv = KeyedVectors.load_word2vec_format(\n",
    "    \"./data/embeddings/glove.6B.100d.txt\", binary=False, no_header=True\n",
    ")\n",
    "embeddings = torch.tensor(embeddings_kv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = utils.Params(os.path.join(model_dir, 'params.json'))\n",
    "checkpoint = torch.load(os.path.join(model_dir, 'best.pth.tar'))\n",
    "\n",
    "model = net.Net(\n",
    "    device=device,\n",
    "    embeddings=embeddings,\n",
    "    num_heads=params.num_heads,\n",
    "    num_layers=params.num_layers,\n",
    "    num_classes=params.num_classes,\n",
    "    input_window_size=params.max_input_length,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(400000, 100)\n",
       "  (encoders): ModuleList(\n",
       "    (0-1): 2 x TransformerEncoder(\n",
       "      (heads): ModuleList(\n",
       "        (0-9): 10 x AttentionHead(\n",
       "          (w_q): Linear(in_features=100, out_features=10, bias=True)\n",
       "          (w_k): Linear(in_features=100, out_features=10, bias=True)\n",
       "          (w_v): Linear(in_features=100, out_features=10, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (fc): Linear(in_features=10, out_features=100, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (linear): Linear(in_features=1000, out_features=100, bias=True)\n",
       "      (ln1): LayerNorm()\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (ff1): Linear(in_features=100, out_features=400, bias=True)\n",
       "      (ff2): Linear(in_features=400, out_features=100, bias=True)\n",
       "      (ln2): LayerNorm()\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember that you must call model.eval() to set dropout and batch normalization\n",
    "# layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input: list[str]) -> torch.LongTensor:\n",
    "    xs = []\n",
    "    for s in input:\n",
    "        ws = [\n",
    "            embeddings_kv.key_to_index[w]\n",
    "            for w in s.split(\" \")\n",
    "            if w in embeddings_kv\n",
    "        ]\n",
    "        # pad with dots (must pad for input to be accepted; since inputs must be of same size)\n",
    "        ws += [embeddings_kv.key_to_index[\".\"]] * (params.max_input_length - len(ws))\n",
    "        xs.append(ws[:params.max_input_length])\n",
    "    return torch.LongTensor(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [\n",
    "    # Positive\n",
    "    'This movie was absolutely amazing! The story was so gripping and the actors gave stellar performances. I highly recommend it.',\n",
    "    'My friend cooked the most delicious dinner last night. The lasagna was made with fresh ingredients and tons of love. I left feeling so satisfied and happy.',\n",
    "    'My new puppy brings me so much joy every day. His curious nature and endless energy are truly uplifting. He makes me smile no matter what.'\n",
    "\n",
    "    # Negative\n",
    "    'That was the worst customer service I\\'ve ever experienced. The staff was rude and unhelpful. I\\'ll never shop there again.',\n",
    "    'Traffic was backed up for miles today. I wasted over an hour just sitting in my car, annoyed and frustrated.',\n",
    "    'This sunset is so dull and boring. The sky is just gray and lifeless with no color whatsoever. What a letdown.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1005,    15,  3960,   523,    15,   100, 20355,     5,     0,  3826,\n",
       "           646, 14063,  1786,  7546,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
       "        [ 1409,  9107,     0,    96, 15476,  3330,    76, 53306,    15,   116,\n",
       "            17,  1903,  7046,     5,  2474,     3,   218,  2518,   100,  5456,\n",
       "             5,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
       "        [   50, 22454,  3998,   285,   100,   181,  5973,   359,  9561,  1746,\n",
       "             5,  9830,   634,    32,  4702,   907,   285,  6107,    84,  1120,\n",
       "            15,     0,  1607,  4188,   270,   661,   821,    15, 16691,     5,\n",
       "           332,  2855,    63,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
       "        [   15,  1837,    60,    10,   679, 10939,    74,    29,  1152,   120,\n",
       "          2995,     6,   192, 18159,     5,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
       "        [10143,    14,   100, 13271,     5,  3505,    14,   120,  3850,     5,\n",
       "         32515,    17,    84,  2487,     7,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenize(input)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "predictions = output.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1038, -5.6546],\n",
       "        [-5.1116, -5.6519],\n",
       "        [-5.1312, -5.6879],\n",
       "        [-5.1081, -5.6466],\n",
       "        [-5.1056, -5.6588]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "05-transformer-encoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
